{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part 1: Transformers for Sequence Classification\n",
                "\n",
                "This notebook reproduces the standard Hugging Face token classification workflow using DistilBERT. It serves as a baseline tutorial to understand how transformer-based NER works before applying these concepts to the more complex Hindi Chunking task in Part 2.\n",
                "\n",
                "The code and data used are the same as in the [token classification tutorial on huggingface](https://huggingface.co/docs/transformers/tasks/token_classification). Minor adjustments were made to manually load the dataset since the original script is deprecated."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import evaluate\n",
                "from datasets import Dataset, DatasetDict, ClassLabel, Features, Sequence, Value\n",
                "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer, pipeline\n",
                "import torch\n",
                "import requests"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load WNUT 17 Dataset (Manual Loading)\n",
                "\n",
                "Since the `wnut_17` dataset script on Hugging Face is no longer supported, the raw files are downloaded and parsed manually."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_wnut_data(url):\n",
                "    response = requests.get(url)\n",
                "    # processing line by line instead of splitting by double newline\n",
                "    lines = response.text.split('\\n')\n",
                "    \n",
                "    tokens_list = []\n",
                "    ner_tags_list = []\n",
                "    \n",
                "    current_tokens = []\n",
                "    current_tags = []\n",
                "    \n",
                "    # Get label list explicitly to map strings to IDs\n",
                "    _label_list = [\n",
                "        \"O\", \"B-corporation\", \"I-corporation\", \"B-creative-work\", \"I-creative-work\", \n",
                "        \"B-group\", \"I-group\", \"B-location\", \"I-location\", \"B-person\", \"I-person\", \n",
                "        \"B-product\", \"I-product\"\n",
                "    ]\n",
                "    label2id = {label: i for i, label in enumerate(_label_list)}\n",
                "    \n",
                "    for line in lines:\n",
                "        # Check if line is empty or just whitespace (some lines are just \\t)\n",
                "        if not line.strip():\n",
                "            if current_tokens:\n",
                "                tokens_list.append(current_tokens)\n",
                "                ner_tags_list.append(current_tags)\n",
                "                current_tokens = []\n",
                "                current_tags = []\n",
                "            continue\n",
                "            \n",
                "        parts = line.split('\\t')\n",
                "        if len(parts) >= 2:\n",
                "            current_tokens.append(parts[0])\n",
                "            tag = parts[1].strip() # Remove any potential trailing whitespace from tag\n",
                "            current_tags.append(label2id.get(tag, 0)) # Default to O if unknown\n",
                "            \n",
                "    # Add last buffer if exists\n",
                "    if current_tokens:\n",
                "        tokens_list.append(current_tokens)\n",
                "        ner_tags_list.append(current_tags)\n",
                "            \n",
                "    return {'tokens': tokens_list, 'ner_tags': ner_tags_list}\n",
                "\n",
                "urls = {\n",
                "    'train': 'https://raw.githubusercontent.com/leondz/emerging_entities_17/master/wnut17train.conll',\n",
                "    'validation': 'https://raw.githubusercontent.com/leondz/emerging_entities_17/master/emerging.dev.conll',\n",
                "    'test': 'https://raw.githubusercontent.com/leondz/emerging_entities_17/master/emerging.test.annotated'\n",
                "}\n",
                "\n",
                "data_splits = {}\n",
                "for split, url in urls.items():\n",
                "    print(f\"Downloading {split}...\")\n",
                "    data_dict = load_wnut_data(url)\n",
                "    data_splits[split] = Dataset.from_dict(data_dict)\n",
                "\n",
                "wnut = DatasetDict(data_splits)\n",
                "print(\"Loaded dataset:\", wnut)\n",
                "\n",
                "# Define features manually for compatibility with the rest of the tutorial\n",
                "label_list = [\n",
                "    \"O\",\n",
                "    \"B-corporation\",\n",
                "    \"I-corporation\",\n",
                "    \"B-creative-work\",\n",
                "    \"I-creative-work\",\n",
                "    \"B-group\",\n",
                "    \"I-group\",\n",
                "    \"B-location\",\n",
                "    \"I-location\",\n",
                "    \"B-person\",\n",
                "    \"I-person\",\n",
                "    \"B-product\",\n",
                "    \"I-product\",\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(wnut[\"train\"][0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preprocess"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def tokenize_and_align_labels(examples):\n",
                "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
                "\n",
                "    labels = []\n",
                "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
                "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
                "        previous_word_idx = None\n",
                "        label_ids = []\n",
                "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
                "            if word_idx is None:\n",
                "                label_ids.append(-100)\n",
                "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
                "                label_ids.append(label[word_idx])\n",
                "            else:\n",
                "                label_ids.append(-100)\n",
                "            previous_word_idx = word_idx\n",
                "        labels.append(label_ids)\n",
                "\n",
                "    tokenized_inputs[\"labels\"] = labels\n",
                "    return tokenized_inputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "seqeval = evaluate.load(\"seqeval\")\n",
                "\n",
                "def compute_metrics(p):\n",
                "    predictions, labels = p\n",
                "    predictions = np.argmax(predictions, axis=2)\n",
                "\n",
                "    true_predictions = [\n",
                "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
                "        for prediction, label in zip(predictions, labels)\n",
                "    ]\n",
                "    true_labels = [\n",
                "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
                "        for prediction, label in zip(predictions, labels)\n",
                "    ]\n",
                "\n",
                "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
                "    return {\n",
                "        \"precision\": results[\"overall_precision\"],\n",
                "        \"recall\": results[\"overall_recall\"],\n",
                "        \"f1\": results[\"overall_f1\"],\n",
                "        \"accuracy\": results[\"overall_accuracy\"],\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "id2label = {i: label for i, label in enumerate(label_list)}\n",
                "label2id = {label: i for i, label in enumerate(label_list)}\n",
                "\n",
                "model = AutoModelForTokenClassification.from_pretrained(\n",
                "    \"distilbert/distilbert-base-uncased\", num_labels=13, id2label=id2label, label2id=label2id\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "training_args = TrainingArguments(\n",
                "    output_dir=\"my_awesome_wnut_model\",\n",
                "    learning_rate=2e-5,\n",
                "    per_device_train_batch_size=16,\n",
                "    per_device_eval_batch_size=16,\n",
                "    num_train_epochs=2,\n",
                "    weight_decay=0.01,\n",
                "    eval_strategy=\"epoch\",\n",
                "    save_strategy=\"epoch\",\n",
                "    load_best_model_at_end=True,\n",
                "    push_to_hub=False, # Disabled as requested\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_wnut[\"train\"],\n",
                "    eval_dataset=tokenized_wnut[\"test\"],\n",
                "    processing_class=tokenizer,\n",
                "    data_collator=data_collator,\n",
                "    compute_metrics=compute_metrics,\n",
                ")\n",
                "\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \"The Golden State Warriors are an American professional basketball team based in San Francisco.\"\n",
                "\n",
                "# Using raw model since we didn't push to hub\n",
                "inputs = tokenizer(text, return_tensors=\"pt\")\n",
                "with torch.no_grad():\n",
                "    logits = model(**inputs.to(model.device)).logits\n",
                "\n",
                "predictions = torch.argmax(logits, dim=2)\n",
                "predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
                "print(predicted_token_class)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}