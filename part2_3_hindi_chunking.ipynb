{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part 2 & 3: Hindi Chunking and Performance Analysis\n",
                "\n",
                "This notebook implements shallow parsing (chunking) on a Hindi CoNLL-U dataset using DistilBERT. While the logic builds upon the token classification workflow established in the Hugging Face tutorial (Part 1), significant modifications were necessary to handle the custom dataset format and the specific requirements of the Hindi Chunking task.\n",
                "\n",
                "**Dataset Credit**: The dataset was provided by the university server (Hindi HDTB-UD).\n",
                "**Model Credit**: The fine-tuned Hindi model used in Experiment 3 was provided by `mirfan899` on Hugging Face.\n",
                "\n",
                "### Overview of Changes from Part 1\n",
                "- **Data Loading**: Unlike Part 1, which downloaded WNUT data from a URL using `requests`, this notebook reads local `.conllu` files. Custom parsing logic was written to extract `ChunkId` and `ChunkType` from column 9.\n",
                "- **Label Schema**: The raw data does not provide IOB tags. Custom logic was implemented to convert raw chunk IDs (e.g., `NP`) into standard IOB format (`B-NP`, `I-NP`) by detecting boundary changes.\n",
                "- **Task**: The task is shifted from Named Entity Recognition (NER) to Chunking.\n",
                "\n",
                "### Experiments\n",
                "Three models are compared to analyze the impact of language specificity:\n",
                "1. `distilbert-base-multilingual-cased`: A general multilingual baseline.\n",
                "2. `distilbert-base-uncased`: An English-only baseline (expected to perform poorly, serving as a negative control).\n",
                "3. `mirfan899/hindi-distilbert-ner`: A model fine-tuned on Hindi, expected to offer the best performance.\n",
                "\n",
                "Additionally, a **Joint Classification** task is implemented to predict both Chunk ID (NP, VP) and Chunk Type (Head, Child) simultaneously."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup\n",
                "Standard imports were used. `evaluate` and `transformers` libraries are reused from Part 1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "# Optimizing memory for MPS/Mac environment\n",
                "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
                "\n",
                "import re\n",
                "import numpy as np\n",
                "import evaluate\n",
                "from datasets import Dataset, DatasetDict, ClassLabel, Features, Sequence, Value\n",
                "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
                "import torch"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Loading and Parsing\n",
                "\n",
                "### Custom Parsing Logic\n",
                "In Part 1, the dataset was simple enough to be split effectively by empty lines. Here, the CoNLL-U format is more complex. A custom parser was written to specifically target column 9 (index 9), where the chunk information is hidden.\n",
                "\n",
                "**Logic for Chunk Isolation**:\n",
                "Raw Chunk IDs often contain numbers (e.g., `NP2`) to distinguish them from adjacent chunks of the same type (e.g., `NP1 NP2`). \n",
                "If we stripped these numbers immediately, `NP1` and `NP2` would merge into a single continuous `NP` chunk. Therefore, the **raw IDs are preserved** during this parsing stage to allow for accurate boundary detection (`B-` vs `I-`) in the Label Generation step later."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def parse_conllu(filepath):\n",
                "    sentences = []\n",
                "    with open(filepath, 'r', encoding='utf-8') as f:\n",
                "        lines = f.readlines()\n",
                "\n",
                "    tokens = []\n",
                "    chunk_ids = []\n",
                "    chunk_types = []\n",
                "\n",
                "    for line in lines:\n",
                "        line = line.strip()\n",
                "        # Skip comments and empty lines\n",
                "        if not line or line.startswith('#'):\n",
                "            if tokens:\n",
                "                sentences.append({'tokens': tokens, 'chunk_ids': chunk_ids, 'chunk_types': chunk_types})\n",
                "                tokens = []\n",
                "                chunk_ids = []\n",
                "                chunk_types = []\n",
                "            continue\n",
                "        \n",
                "        parts = line.split('\\t')\n",
                "        if len(parts) < 10:\n",
                "            continue\n",
                "            \n",
                "        token = parts[1]\n",
                "        misc = parts[9]\n",
                "        \n",
                "        # Extract ChunkId and ChunkType from column 9\n",
                "        # Example format: ChunkId=NP|ChunkType=child|...\n",
                "        c_id = 'O'\n",
                "        c_type = 'O'\n",
                "        \n",
                "        misc_parts = misc.split('|')\n",
                "        for mp in misc_parts:\n",
                "            if mp.startswith('ChunkId='):\n",
                "                c_id = mp.split('=')[1] \n",
                "                # Critical: Numbers are NOT stripped yet. 'NP2' is kept as 'NP2'.\n",
                "            elif mp.startswith('ChunkType='):\n",
                "                c_type = mp.split('=')[1]\n",
                "        \n",
                "        tokens.append(token)\n",
                "        chunk_ids.append(c_id)\n",
                "        chunk_types.append(c_type)\n",
                "\n",
                "    # Append final sentence\n",
                "    if tokens:\n",
                "        sentences.append({'tokens': tokens, 'chunk_ids': chunk_ids, 'chunk_types': chunk_types})\n",
                "        \n",
                "    return sentences\n",
                "\n",
                "data_dir = \"data/a2\"\n",
                "train_data = parse_conllu(os.path.join(data_dir, \"hi_hdtb-ud-train.conllu\"))\n",
                "dev_data = parse_conllu(os.path.join(data_dir, \"hi_hdtb-ud-dev.conllu\"))\n",
                "test_data = parse_conllu(os.path.join(data_dir, \"hi_hdtb-ud-test.conllu\"))\n",
                "\n",
                "print(f\"Loaded {len(train_data)} training sentences.\")\n",
                "print(f\"Loaded {len(dev_data)} validation sentences.\")\n",
                "print(f\"Loaded {len(test_data)} test sentences.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Data Inspection\n",
                "A quick check was performed to identify the distribution of 'O' (Outside) tags, giving insight into how sparse the chunks are."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def check_o_tags(sentences):\n",
                "    total_tokens = 0\n",
                "    o_tags = 0\n",
                "    for sent in sentences:\n",
                "        total_tokens += len(sent['chunk_ids'])\n",
                "        o_tags += sent['chunk_ids'].count('O')\n",
                "    \n",
                "    print(f\"Total Tokens: {total_tokens}\")\n",
                "    print(f\"Tokens with 'O' ChunkID: {o_tags}\")\n",
                "    print(f\"Percentage 'O': {o_tags/total_tokens*100:.2f}%\")\n",
                "\n",
                "print(\"--- Train Data ---\")\n",
                "check_o_tags(train_data)\n",
                "print(\"Sample:\", train_data[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Label Generation (IOB + Joint Tags)\n",
                "\n",
                "### Why Custom Logic?\n",
                "Unlike the WNUT dataset in Part 1, which came with pre-defined NER tags (`B-corporation`, etc.), this dataset only provides raw IDs (`NP`, `VP`). To use standard token classification models, IOB (Inside-Outside-Beginning) tags must be generated programmatically.\n",
                "\n",
                "**Algorithm Description**:\n",
                "The function iterates through the tokens and compares the current `ChunkId` with the previous one.\n",
                "1. **B-Tag (Beginning)**: Assigned if the current ID differs from the previous one (e.g., switching from `NP1` to `NP2`, or `O` to `NP`).\n",
                "2. **I-Tag (Inside)**: Assigned if the current ID matches the previous one (e.g., `NP1` follows `NP1`).\n",
                "3. **Number Stripping**: Once the prefix (B/I) is determined, the trailing numbers are removed (`NP2` -> `NP`) to create generalizable classes.\n",
                "4. **Joint Classification (Bonus)**: The label is constructed by combining the IOB tag, the stripped Chunk ID, and the Chunk Type: `B-NP-head`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_labels(sentences):\n",
                "    processed = []\n",
                "    all_labels = set()\n",
                "    \n",
                "    for sent in sentences:\n",
                "        tokens = sent['tokens']\n",
                "        c_ids = sent['chunk_ids']\n",
                "        c_types = sent['chunk_types']\n",
                "        \n",
                "        ner_tags = []\n",
                "        \n",
                "        for i, (cid, ctype) in enumerate(zip(c_ids, c_types)):\n",
                "            if cid in ['O', 'BLK', '']: # Handling punctuation/blanks as Outside\n",
                "                label = 'O'\n",
                "                ner_tags.append(label)\n",
                "                all_labels.add(label)\n",
                "                continue\n",
                "            \n",
                "            # Determine Prefix (B vs I)\n",
                "            # Start of a chunk if:\n",
                "            # 1. It is the first token\n",
                "            # 2. OR the current Raw ChunkID is different from the previous one\n",
                "            if i == 0 or cid != c_ids[i-1]:\n",
                "                prefix = 'B'\n",
                "            else:\n",
                "                prefix = 'I'\n",
                "            \n",
                "            # Strip number from ChunkId for the final label\n",
                "            # e.g. NP2 -> NP\n",
                "            clean_cid = re.sub(r'\\d+$', '', cid)\n",
                "            \n",
                "            # Constructing the Joint Label\n",
                "            label = f\"{prefix}-{clean_cid}-{ctype}\"\n",
                "            \n",
                "            ner_tags.append(label)\n",
                "            all_labels.add(label)\n",
                "            \n",
                "        processed.append({'tokens': tokens, 'ner_tags': ner_tags})\n",
                "        \n",
                "    return processed, sorted(list(all_labels))\n",
                "\n",
                "train_processed, label_list = generate_labels(train_data)\n",
                "dev_processed, _ = generate_labels(dev_data)\n",
                "test_processed, _ = generate_labels(test_data)\n",
                "\n",
                "label2id = {label: i for i, label in enumerate(label_list)}\n",
                "id2label = {i: label for i, label in enumerate(label_list)}\n",
                "\n",
                "print(\"Labels:\", label_list)\n",
                "print(\"Sample processed:\", train_processed[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dataset Conversion\n",
                "The processed data is converted into the Hugging Face `Dataset` library format. This standardizes the input for the Trainer API used in subsequent steps."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def convert_to_hf_dataset(processed_data, label2id):\n",
                "    hf_data = {\n",
                "        'tokens': [],\n",
                "        'ner_tags': []\n",
                "    }\n",
                "    for item in processed_data:\n",
                "        hf_data['tokens'].append(item['tokens'])\n",
                "        tags_ids = [label2id[tag] for tag in item['ner_tags']]\n",
                "        hf_data['ner_tags'].append(tags_ids)\n",
                "    return Dataset.from_dict(hf_data)\n",
                "\n",
                "hf_train = convert_to_hf_dataset(train_processed, label2id)\n",
                "hf_dev = convert_to_hf_dataset(dev_processed, label2id)\n",
                "hf_test = convert_to_hf_dataset(test_processed, label2id)\n",
                "\n",
                "dataset = DatasetDict({\n",
                "    'train': hf_train,\n",
                "    'validation': hf_dev,\n",
                "    'test': hf_test\n",
                "})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluation Metrics\n",
                "The `seqeval` library is used for standard token classification metrics (Precision, Recall, F1). The `compute_metrics` function was reused directly from Part 1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "seqeval = evaluate.load(\"seqeval\")\n",
                "\n",
                "def compute_metrics(p):\n",
                "    predictions, labels = p\n",
                "    predictions = np.argmax(predictions, axis=2)\n",
                "    \n",
                "    # Convert IDs back to labels, filtering out -100 (special tokens)\n",
                "    true_predictions = [\n",
                "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
                "        for prediction, label in zip(predictions, labels)\n",
                "    ]\n",
                "    true_labels = [\n",
                "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
                "        for prediction, label in zip(predictions, labels)\n",
                "    ]\n",
                "\n",
                "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
                "    return {\n",
                "        \"precision\": results[\"overall_precision\"],\n",
                "        \"recall\": results[\"overall_recall\"],\n",
                "        \"f1\": results[\"overall_f1\"],\n",
                "        \"accuracy\": results[\"overall_accuracy\"],\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokenization Helper\n",
                "\n",
                "### Reused Logic from Part 1\n",
                "The `tokenize_and_align_labels` function (encapsulated here within `tokenize_dataset` for cleaner experimentation) is reused from the Hugging Face tutorial code in Part 1. \n",
                "\n",
                "**Purpose**: Transformers use subword tokenization (WordPiece), meaning one word might be split into multiple tokens. We must align the single label provided for the word (e.g., `B-NP`) to these multiple tokens. Following the tutorial strategy, the label is assigned to the **first subword**, and subsequent subwords are ignored (set to `-100`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def tokenize_dataset(dataset, tokenizer_checkpoint):\n",
                "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)\n",
                "    \n",
                "    def align_labels(examples):\n",
                "        tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
                "        labels = []\n",
                "        for i, label in enumerate(examples[\"ner_tags\"]):\n",
                "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
                "            previous_word_idx = None\n",
                "            label_ids = []\n",
                "            for word_idx in word_ids:\n",
                "                if word_idx is None:\n",
                "                    label_ids.append(-100)\n",
                "                elif word_idx != previous_word_idx:\n",
                "                    label_ids.append(label[word_idx])\n",
                "                else:\n",
                "                    label_ids.append(-100)\n",
                "                previous_word_idx = word_idx\n",
                "            labels.append(label_ids)\n",
                "        tokenized_inputs[\"labels\"] = labels\n",
                "        return tokenized_inputs\n",
                "\n",
                "    return dataset.map(align_labels, batched=True), tokenizer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Experiment 1: Multilingual Baseline\n",
                "\n",
                "DistilBERT Multilingual (`distilbert-base-multilingual-cased`) was selected as the baseline. It supports Hindi and provides a reasonable starting point for performance comparison."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CHECKPOINT_MULTI = \"distilbert-base-multilingual-cased\"\n",
                "\n",
                "# 1. Prepare Data\n",
                "tokenized_multi, tokenizer_multi = tokenize_dataset(dataset, CHECKPOINT_MULTI)\n",
                "data_collator_multi = DataCollatorForTokenClassification(tokenizer=tokenizer_multi)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Initialize Model\n",
                "model_multi = AutoModelForTokenClassification.from_pretrained(\n",
                "    CHECKPOINT_MULTI, \n",
                "    num_labels=len(label_list), \n",
                "    id2label=id2label, \n",
                "    label2id=label2id\n",
                ")\n",
                "\n",
                "# Clear MPS cache (Mac usage)\n",
                "if torch.backends.mps.is_available():\n",
                "    torch.mps.empty_cache()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Train\n",
                "args_multi = TrainingArguments(\n",
                "    output_dir=\"results_multi_cased\",\n",
                "    learning_rate=2e-5,\n",
                "    per_device_train_batch_size=4,\n",
                "    per_device_eval_batch_size=4,\n",
                "    gradient_accumulation_steps=4,\n",
                "    num_train_epochs=3,\n",
                "    weight_decay=0.01,\n",
                "    eval_strategy=\"epoch\",\n",
                "    save_strategy=\"epoch\",\n",
                "    load_best_model_at_end=True,\n",
                "    push_to_hub=False,\n",
                "    logging_steps=50,\n",
                "    group_by_length=True \n",
                ")\n",
                "\n",
                "trainer_multi = Trainer(\n",
                "    model=model_multi,\n",
                "    args=args_multi,\n",
                "    train_dataset=tokenized_multi[\"train\"],\n",
                "    eval_dataset=tokenized_multi[\"validation\"],\n",
                "    compute_metrics=compute_metrics,\n",
                "    data_collator=data_collator_multi,\n",
                "    processing_class=tokenizer_multi,\n",
                ")\n",
                "\n",
                "trainer_multi.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Evaluate\n",
                "results_multi_eval = trainer_multi.evaluate(tokenized_multi[\"test\"])\n",
                "print(\"Multilingual Model Results:\", results_multi_eval)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Experiment 2: English Baseline\n",
                "\n",
                "DistilBERT (`distilbert-base-uncased`) was trained solely on English data. This experiment was included to quantify the difficulty of the task; if an English-only model performs well, the task may rely more on structure/punctuation than language semantics. We expect this model to perform poorly on Hindi text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CHECKPOINT_EN = \"distilbert-base-uncased\"\n",
                "\n",
                "# 1. Prepare Data\n",
                "tokenized_en, tokenizer_en = tokenize_dataset(dataset, CHECKPOINT_EN)\n",
                "data_collator_en = DataCollatorForTokenClassification(tokenizer=tokenizer_en)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Initialize Model\n",
                "model_en = AutoModelForTokenClassification.from_pretrained(\n",
                "    CHECKPOINT_EN, \n",
                "    num_labels=len(label_list), \n",
                "    id2label=id2label, \n",
                "    label2id=label2id\n",
                ")\n",
                "\n",
                "if torch.backends.mps.is_available():\n",
                "    torch.mps.empty_cache()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Train\n",
                "args_en = TrainingArguments(\n",
                "    output_dir=\"results_en_uncased\",\n",
                "    learning_rate=2e-5,\n",
                "    per_device_train_batch_size=4,\n",
                "    per_device_eval_batch_size=4,\n",
                "    gradient_accumulation_steps=4,\n",
                "    num_train_epochs=3,\n",
                "    weight_decay=0.01,\n",
                "    eval_strategy=\"epoch\",\n",
                "    save_strategy=\"epoch\",\n",
                "    load_best_model_at_end=True,\n",
                "    push_to_hub=False,\n",
                "    logging_steps=50,\n",
                "    group_by_length=True\n",
                ")\n",
                "\n",
                "trainer_en = Trainer(\n",
                "    model=model_en,\n",
                "    args=args_en,\n",
                "    train_dataset=tokenized_en[\"train\"],\n",
                "    eval_dataset=tokenized_en[\"validation\"],\n",
                "    compute_metrics=compute_metrics,\n",
                "    data_collator=data_collator_en,\n",
                "    processing_class=tokenizer_en,\n",
                ")\n",
                "\n",
                "trainer_en.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Evaluate\n",
                "results_en_eval = trainer_en.evaluate(tokenized_en[\"test\"])\n",
                "print(\"English Model Results:\", results_en_eval)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Experiment 3: Fine-tuned Hindi Model\n",
                "\n",
                "The model `mirfan899/hindi-distilbert-ner` was chosen as it has been fine-tuned on Hindi data. It is expected to understand the specific linguistic features of Hindi better than the generic multilingual model. `ignore_mismatched_sizes=True` is used because we are replacing the original NER classification head with our new 31-class Chunking head."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CHECKPOINT_HINDI = \"mirfan899/hindi-distilbert-ner\"\n",
                "\n",
                "# 1. Prepare Data\n",
                "tokenized_hindi, tokenizer_hindi = tokenize_dataset(dataset, CHECKPOINT_HINDI)\n",
                "data_collator_hindi = DataCollatorForTokenClassification(tokenizer=tokenizer_hindi)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Initialize Model\n",
                "# Added ignore_mismatched_sizes=True to handle the new label head\n",
                "model_hindi = AutoModelForTokenClassification.from_pretrained(\n",
                "    CHECKPOINT_HINDI, \n",
                "    num_labels=len(label_list), \n",
                "    id2label=id2label, \n",
                "    label2id=label2id,\n",
                "    ignore_mismatched_sizes=True\n",
                ")\n",
                "\n",
                "if torch.backends.mps.is_available():\n",
                "    torch.mps.empty_cache()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Train\n",
                "args_hindi = TrainingArguments(\n",
                "    output_dir=\"results_hindi_ft\",\n",
                "    learning_rate=2e-5,\n",
                "    per_device_train_batch_size=4,\n",
                "    per_device_eval_batch_size=4,\n",
                "    gradient_accumulation_steps=4,\n",
                "    num_train_epochs=3,\n",
                "    weight_decay=0.01,\n",
                "    eval_strategy=\"epoch\",\n",
                "    save_strategy=\"epoch\",\n",
                "    load_best_model_at_end=True,\n",
                "    push_to_hub=False,\n",
                "    logging_steps=50,\n",
                "    group_by_length=True\n",
                ")\n",
                "\n",
                "trainer_hindi = Trainer(\n",
                "    model=model_hindi,\n",
                "    args=args_hindi,\n",
                "    train_dataset=tokenized_hindi[\"train\"],\n",
                "    eval_dataset=tokenized_hindi[\"validation\"],\n",
                "    compute_metrics=compute_metrics,\n",
                "    data_collator=data_collator_hindi,\n",
                "    processing_class=tokenizer_hindi,\n",
                ")\n",
                "\n",
                "trainer_hindi.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Evaluate\n",
                "results_hindi_eval = trainer_hindi.evaluate(tokenized_hindi[\"test\"])\n",
                "print(\"Hindi Model Results:\", results_hindi_eval)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualization\n",
                "\n",
                "To better understand the model's behavior, random predictions from the test set are visualized. This qualitative check helps verify if the model produces sane outputs (e.g., contiguous `B-NP` followed by `I-NP`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import random\n",
                "\n",
                "def show_random_predictions(model, tokenizer, dataset, num_examples=10):\n",
                "    print(f\"\\n--- Random Predictions for {model.name_or_path} ---\")\n",
                "    \n",
                "    # Pick random indices\n",
                "    indices = random.sample(range(len(dataset)), num_examples)\n",
                "    \n",
                "    for idx in indices:\n",
                "        example = dataset[idx]\n",
                "        tokens = example['tokens']\n",
                "        labels = example['ner_tags'] # These are IDs\n",
                "        \n",
                "        # Tokenize (Keep the BatchEncoding object to get word_ids)\n",
                "        tokenized_input = tokenizer(tokens, truncation=True, is_split_into_words=True, return_tensors=\"pt\")\n",
                "        \n",
                "        # Move inputs to device\n",
                "        inputs = {k: v.to(model.device) for k,v in tokenized_input.items()}\n",
                "        \n",
                "        # Predict\n",
                "        with torch.no_grad():\n",
                "            outputs = model(**inputs)\n",
                "        \n",
                "        logits = outputs.logits\n",
                "        predictions = torch.argmax(logits, dim=2)[0].cpu().numpy()\n",
                "        \n",
                "        # Align back to words\n",
                "        # word_ids() works on the BatchEncoding object (tokenized_input), NOT the dict\n",
                "        word_ids = tokenized_input.word_ids()\n",
                "        aligned_preds = []\n",
                "        aligned_labels = []\n",
                "        seen_words = set()\n",
                "        \n",
                "        for i, word_id in enumerate(word_ids):\n",
                "            if word_id is None: continue\n",
                "            if word_id not in seen_words:\n",
                "                # First token of the word -> keep prediction\n",
                "                pred_label = id2label[predictions[i]]\n",
                "                true_label = id2label[labels[word_id]] if labels[word_id] != -100 else \"O\"\n",
                "                \n",
                "                aligned_preds.append(pred_label)\n",
                "                aligned_labels.append(true_label)\n",
                "                seen_words.add(word_id)\n",
                "        \n",
                "        # Display\n",
                "        print(f\"\\nSentence: {' '.join(tokens)}\")\n",
                "        print(f\"Predicted: {aligned_preds}\")\n",
                "        print(f\"True:      {aligned_labels}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "show_random_predictions(model_multi, tokenizer_multi, hf_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "show_random_predictions(model_en, tokenizer_en, hf_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "show_random_predictions(model_hindi, tokenizer_hindi, hf_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary of Results\n",
                "The performance of all three models is aggregated below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "summary = {\n",
                "    \"Model\": [\"Multilingual\", \"English (Uncased)\", \"Hindi (Fine-tuned)\"],\n",
                "    \"Precision\": [results_multi_eval['eval_precision'], results_en_eval['eval_precision'], results_hindi_eval['eval_precision']],\n",
                "    \"Recall\": [results_multi_eval['eval_recall'], results_en_eval['eval_recall'], results_hindi_eval['eval_recall']],\n",
                "    \"F1\": [results_multi_eval['eval_f1'], results_en_eval['eval_f1'], results_hindi_eval['eval_f1']],\n",
                "    \"Accuracy\": [results_multi_eval['eval_accuracy'], results_en_eval['eval_accuracy'], results_hindi_eval['eval_accuracy']]\n",
                "}\n",
                "\n",
                "df = pd.DataFrame(summary)\n",
                "print(df)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}